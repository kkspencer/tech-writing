{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hi there! \u00b6 Technical Writing Samples A quickstart guide to Go Modules Getting Started with Cuelang Authoring Avro schemas with Avro IDL Did you find this page useful? Yes! No Maintained by: Kelvin Spencer","title":"Home"},{"location":"#hi-there","text":"Technical Writing Samples A quickstart guide to Go Modules Getting Started with Cuelang Authoring Avro schemas with Avro IDL Did you find this page useful? Yes! No Maintained by: Kelvin Spencer","title":"Hi there!"},{"location":"authoring-schemas-with-avro-idl/","text":"Apache Avro is a binary encoding format that uses a schema to specify the structure of the data being encoded. The Avro IDL (Interface Description Language) is a high level language for authoring Avro schemas. This allows developers to author schemas in a way that feels familiar to writing code, instead of the error-prone process of filling out a JSON schema by hand. In this documentation, we will focus on common Avro IDL features. Content Usage Data Types Naming Conventions Annotations, Comments, and other language constructs Schema Imports An IDL is not unique to Avro It is important to note that an Interface Description Language (IDL) is simply a descriptive language used to define data types and interfaces in a way that is independent of a programming language or platform. An IDL specifies only the syntax used to define the data types and interfaces. Usage \u00b6 In this section, we will walk through the creation of an event schema using the Avro IDL , generate the corresponding Avro JSON schema which can then be used by the avrogo tool to generate Go data structures for your services. This should provide the necessary context before diving into the specifics of authoring schemas with the Avro IDL . Avro IDL Schema declaration First, make sure you have a version of java (such as OpenJDK ) installed on your machine Download and install the avro-tools Check that the avro-tools have been properly installed by running java -jar avro-tools-{$version}.jar where {$version} is the version of the avro-tools you just downloaded Example: Schema declaration for a service Schema with imported headers @ namespace ( \"service_name\" ) protocol service_event { /* headers.avdl contains the standard headers that must be included with every avro event. */ import idl \"../headers.avdl\" ; /* add other schema imports if needed import idl \"my_other_service.avdl\"; import schema \"my_other_service.avsc\" */ @ meta ({ \"commentary\" : \"This Schema describes version v0.0.0 of the event service_event from the service_name.\" , \"version\" : \"0.0.0\" , \"event\" : \"service_event\" , \"eventDescription\" : \"TODO: Please describe the service_event here.\" , \"eventTriggeredBy\" : \"TODO: Please desrcibe the circumstances under which the service_event event is triggered, here.\" , \"status\" : \"active\" , \"partitions\" : \"1\" , \"topickey\" : \"service_name.service_event\" }) /* Put your enum definitions here. */ record service_event { /* Do not remove the following line */ meta Metadata ; /* Put your payload definition here. */ } Add service-specific data to the schema template Update schema template with service data @ namespace ( \"service_name\" ) protocol service_event { /* headers.avdl contains the standard headers that must be included with every avro event. */ import idl \"headers.avdl\" ; @ meta ({ \"commentary\" : \"This Schema describes version v0.0.0 of the event service_event from the service_name.\" , \"version\" : \"0.0.0\" , \"event\" : \"service_event\" , \"eventDescription\" : \"TODO: Please describe the service_event event here.\" , \"eventTriggeredBy\" : \"TODO: Please desrcibe the circumstances under which the service_event event is triggered, here.\" , \"status\" : \"active\" , \"partitions\" : \"1\" , \"topickey\" : \"service_name.service_event\" }) /* Put your enum definitions here. */ enum Type { payment , payment_adjustment , direct_payment } enum Mode { cash , card , in_car_with_fee , paid_by_platform } record service_event { /* Do not remove the following line */ meta Metadata ; /* Put your payload definition here. */ Type PaymentType ; Mode PaymentMode ; string ID ; union { null , string } TransactionID ; // optional string RideID ; string BusinessEventName ; decimal ( 4 , 2 ) ChargedAmount ; decimal ( 4 , 2 ) PlatformFee ; decimal ( 3 , 2 ) DiscountAmount ; string Currency ; string Country ; union { null , string } Reason ; // optional @ TimeStamp ( \"timestamp-micros\" ) // using a custom annotation to specify timestamp-micros long CreatedAt ; timestamp_ms UpdatedAt ; // using the built-in Avro IDL timestamp_ms logicalType string ` error ` ; // using an escaped Avro IDL keyword } Generate Avro Schema file (`.avsc`) files from the `.avdl` file using the `idl2schemata` command java -jar avro-tools-{$version}.jar idl2schemata example_schema.avdl The new Avro Schema file ( .avsc ) will be generated in the same folder as the .avdl file Bonus You can also generate Go data structures from the .avsc file using the avrogo command Data Types \u00b6 As with programming languages, the AVRO IDL defines data types that can be used as building blocks for authoring Avro schemas. The AVRO IDL defines: primitive data types complex data types logical types Reader's Schema vs Writer's Schema Because a schema is used to tell the datatype of each field, binary data can only be decoded correctly if the code reading the data knows the exact schema that the data was written with. If it uses a different schema, arbitrary data corruption could result. When an application wants to encode data, it encodes the data using whatever schema version it has. This is known as the writer's schema . A header is added to the data that encodes a schema id for the schema, which is found by querying the schema registry. This means that an application producing Avro messages must be using a schema that is already registered. When an application wants to decode data, it reads it into its own schema, known as the reader's schema . This does not have to be the same as the writer's schema, but it does have to be compatible . The process of transforming from the writer's schema to the reader's schema is known as schema resolution and full details are available here . The key idea with Avro is that the writer's schema and the reader's schema do not have to be the same, they only need to be compatible. Primitive Data Types \u00b6 In Avro , Primitive types have no specified attributes, and their names may not be defined in any namespace. As such, primitive types automatically qualify as defined type names. Avro IDL declaration for `string` data type string ID ; Equivalent Avro JSON schema (`.avsc`) for `string` data type { \"name\" : \"ID\" , \"type\" : \"string\" } The Avro IDL supports the following primitive types: Avro IDL declaration Equivalent Avro JSON schema Type Description null null no value boolean boolean binary value int int 32-bit signed integer long long 64-bit signed integer float float single precision (32-bit) IEEE 754 floating-point number double double double precision (64-bit) IEEE 754 floating-point number bytes bytes sequence of 8-bit unsigned bytes string string unicode character sequence Complex Data Types \u00b6 Avro supports the following complex types: records enums arrays maps unions fixed Records \u00b6 Record Types Records are like structs in the sense that they can be composed of both primitive data types and complex data types. Avro IDL declaration for a record type record Service { string service_name ; boolean is_active ; timestamp - millis created_at ; }; Equivalent Avro JSON schema { \"type\" : \"record\" , \"name\" : \"Service\" , \"fields\" : [ { \"name\" : \"service_name\" , \"type\" : \"string\" }, { \"name\" : \"is_active\" , \"type\" : \"boolean\" } { \"name\" : \"created_at\" , \"type\" : \"long\" , \"logicalType\" : \"timestamp-millis\" } ] } Not all complex types can be embedded in a Record Though the Avro specifications suggest that complex types such as enum types and even record types can be embedded within a record , the Avro IDL does not currently support these types of embeddings. With the current implementation, record types can only contain the following types: array boolean double float int long map bytes string null union date time_ms timestamp_ms Enums \u00b6 Enum Types Enumerated types are declared using the enum keyword. For example, to declare an enum describing a suit of playing cards: Avro IDL declaration for an enum type enum Suit { SPADES , HEARTS , DIAMONDS , CLUBS } Equivalent Avro JSON schema { \"type\" : \"enum\" , \"name\" : \"Suit\" , \"symbols\" : [ \"SPADES\" , \"HEARTS\" , \"DIAMONDS\" , \"CLUBS\" ] } Arrays \u00b6 Array Types The array type can be used to declare a list of items defined by the same attribute. Example: array of strings Avro IDL declaration for array of strings array < string > mountain_ranges ; Equivalent Avro JSON schema { \"type\" : \"array\" , \"name\" : \"mountain_ranges\" , \"items\" : \"string\" } Example: array of ints Avro IDL declaration for array of ints array < int > number_bases ; Equivalent Avro JSON schema { \"type\" : \"array\" , \"name\" : \"number_bases\" , \"items\" : \"int\" } Maps \u00b6 Map Types The map type can be used to declare a list of key-value pairs, where all keys are constrained to be string types. Example: Map with strings as keys (by default), and longs as values Avro IDL declaration for a map with `longs` as values map < long > astronomical_units_to_meters ; Equivalent Avro JSON schema { \"type\" : \"map\" , \"name\" : \"astronomical_units_to_meters\" , \"values\" : \"long\" } Unions \u00b6 Union Types union types are often used to declare fields with optional values Example: union declaring a schema which may be either a null type or a string type Avro IDL declaration for an optional string union { null , string } optionalString ; Equivalent Avro JSON schema { \"name\" : \"optionalString\" , [ \"null\" , \"string\" ] } Rules When a default value is specified for a record whose type is a union , the type of the default value must match the first element of the union . Thus for unions containing null , the null SHOULD be listed first since the default value of such unions is typically null Restrictions on Combining unions unions may not immediately contain other unions unions may not contain more than one schema with the same type except for record types, fixed types and enum types unions containing two array types or two map types are not permitted, but two types with different names are permitted. This is because names permit efficient resolution when reading and writing unions . Fixed \u00b6 Fixed Types fixed types are user-defined types where the specific number of bytes required to hold the type is explicitly declared. The number of bytes is specified as a positive integer. Example: a 16-byte quantity for holding an md5 hash Avro IDL declaration for an MD5 hash type fixed MD5 ( 16 ); Equivalent Avro JSON schema { \"type\" : \"fixed\" , \"size\" : 16 , \"name\" : \"MD5\" } Logical Types \u00b6 A logical type is an Avro primitive or complex type with extra attributes to represent a derived type. The attribute logicalType must always be present for a logical type. It is a string representation of one of these logical types: decimal uuid date time-micros timestamp-micros time-millis timestamp-millis Only timestamp-millis and time-millis and date supported In the current implementation of the Avro IDL , only the timestamp-millis , time-millis and date logicalType are supported. The other logicalTypes are supported in Avro JSON schemas, but cannot be generated with the Avro IDL . Serialization of logicalType A logical type is always serialized using its underlying Avro type so that values are encoded in exactly the same way as the equivalent Avro type that does not have a logicalType attribute. Thus, if a logical type is invalid, for example a decimal with scale greater than its precision, then implementations COULD ignore the logical type and use the underlying Avro type. Decimal Logical Type Not Yet supported in Avro IDL The decimal logical type represents an arbitrary-precision signed decimal number of the form: unscaled \u00d7 10-scale. Example: schema representing decimal numbers with a maximum precision of 4 and a scale of 2 Avro IDL declaration for the decimal logical type decimal ( 4 , 2 ) dailyEarnings ; Equivalent Avro JSON schema { \"type\" : \"bytes\" , // underlying Avro type \"logicalType\" : \"decimal\" , \"name\" : \"dailyEarnings\" , \"precision\" : 4 , \"scale\" : 2 } Attributes Required or Optional? Description precision REQUIRED the MAXIMUM number of digits that can be represented, irrespective of the position of the decimal point scale OPTIONAL the number of fractional digits. If not specified, the scale is defined as 0 Ex: Given the declaration: decimal(4) dailyEarnings; The following decimals are all valid: 9999 -9999 ... and all other digits between {-9999, 9999} A scale value of 0 resolves to integer values since there are no fractional digits Restrictions precision must be a positive integer (greater than zero). If the underlying type is a fixed type, then the precision is limited by its size scale must be 0 or a positive integer less than or equal to the precision Ex: Given the declaration: decimal(4, 2) rideFees; The following decimals are all valid: 99.99 -99.99 ... and all other decimals between {-99.99, 99.99} A scale value equal to the precision value resolves to a decimal with only fractional digits Ex: Given the declaration: decimal (4, 4) commission; The following decimals are valid: 0.999 -0.999 ... and all other decimals between {-0.999, 0.999} When resolving schemas, two schemas that are decimal logical types match if their scales and precisions match UUID Logical Type Not Yet supported in Avro IDL The uuid logical type represents a random generated, universally unique identifier (UUID). It annotates an Avro string type that conforms with RFC-4122 Example: named schema for representing a user_id with a UUID logical type Avro IDL declaration for a UUID logical type UUID user_id ; Equivalent Avro JSON schema { \"name\" : \"user_id\" , \"type\" : \"string\" , \"logicalType\" : \"UUID\" } Date Logical Type Not Yet supported in Avro IDL The date logical type represents a date, with NO reference to time zone or time of day. It annotates an Avro int type, where the int stores the number of days from the unix epoch (1 January 1970 [ISO Calendar]) Example: named schema for representing April Fool's day with a date logical type Avro IDL declaration for a date logical type date april_fools_day ; Equivalent Avro JSON schema { \"name\" : \"april_fools_day\" , \"type\" : \"int\" , \"logicalType\" : \"date\" } Logical Types for representing Time Logical Type Precision Underlying Avro Type Storage Description time-millis 1 millisecond int number of milliseconds after midnight, 00:00:00.000 represents a time of day, with no reference to a particular calendar, time zone or date time-micros 1 microsecond long numer of microseconds after midnight, 00:00:00.000000 represents a time of day, with no reference to a particular calendar, time zone or date timestamp-millis 1 millisecond long number of milliseconds from the unix epoch, 1 January 1970 00:00:00.000 UTC represents an instant on the global timeline, independent of a particular time zone or calendar timestamp-micros 1 microsecond long number of microseconds from the unix epoch, 1 January 1970 00:00:00.000000 UTC represents an instant on the global timeline, independent of a particular time zone or calendar Example: named timestamp for representing the time a user account was created, using a timestamp-millis logical type Avro IDL declaration for a timestamp-millis logical type timestamp - millis created_at ; Equivalent Avro JSON schema { \"name\" : \"created_at\" , \"type\" : \"long\" , \"logicalType\" : \"timestamp-millis\" } timestamp-micros and time-micros not currently supported in Avro-IDL. Use a custom annotation instead such as: @ serviceTimeStamp ( \"timestamp-micros\" ) // using a custom annotation to specify timestamp-micros long CreatedAt ; Naming Conventions \u00b6 Named Types \u00b6 Defining named types records , enums , and fixed types are named types, where each fullname consists of: name : starts with characters that match [A-Za-z_] , and CAN subsequently contain characters that match [A-Za-z0-9_] namespace : dot-separated sequence of name -like tokens that qualify a name . The empty string can be used to indicate the null namespace Restrictions The uniqueness of the name associated with a named type is defined on the fullname The check for equality on names (including field names and enum symbols) is case-sensitive A schema may not contain multiple definitions of a fullname A name must be defined before it is used Aliases \u00b6 Using aliases for named types and fields Aliases provide a mechanism for flexibility when declaring a schema. They allow named types and fields to be optionally mapped to other named types and fields, which makes it easier for schemas to evolve. Example: specifying type and field aliases, using the @alias annotation @ aliases ([ \"OldRecord\" , \"AncientRecord\" ]) record MyRecord { // map the named record type `MyRecord` to `OldRecord` and `AncientRecord` string @ aliases ([ \"oldField\" , \"ancientField\" ]) myNewField ; // map the record's string field `myNewField` to `oldField` and `ancientField` } Consequently, the named record type MyRecord can also be referenced as OldRecord or AncientRecord . Similarly, MyRecord 's named string field myNewField can also be referenced as oldField or ancientField . In the context of event driven applications, this means that producers can safely update the schemas, without worrying about introducing breaking changes for the consumers , since aliases provide a mechanism for backwards compatibility. Annotations, Comments and Other Language Constructs \u00b6 The annotation tag ( @ ) is used to specify certain properties of the annotated element. Here are some common annotations: annotation description @namespace specify namespace of a record , or modify the namespace when defininig a named schema @aliases specify type and field aliases of a record Examples of annotations Modify the namespace when defining a named schema, using the @namespace annotation @ namespace ( \"legacy.bookkeeping\" ) protocol transaction_created { @ namespace ( \"current.bookkeeping\" ) record current_transaction {} record legacy_transaction {} } where the transaction_created protocol is defined in the legacy.bookkeeping namespace, the record current_transaction is defined in the current.bookkeeping namespace and the record legacy_transaction is defined in the legacy.bookkeeping namespace, since it inherits its default namespace from its container. annotations can be used to include arbitrary JSON elements JSON elements that are not directly supported in the Avro IDL , such as service-specific metadata, or certain logical types such as timestamp-micros can be declared using annotation tags. For example: Declaring a service-specific timestamp using annotations @ serviceTimeStamp ( \"timestamp-micros\" ) // using the generic annotation to specify timestamp-micros long CreatedAt ; Equivalent Avro JSON format { \"name\" : \"CreatedAt\" , \"type\" : { \"type\" : \"long\" , \"serviceTimeStamp\" : \"timestamp-micros\" } } Comments The Avro IDL supports C -style comments, where: any text following // on a line is ignored any text between /* and */ (possibly spanning multiple lines) is ignored Using reserved language keywords as identifiers In the rare occasion that a reserved language keyword needs to be used as an identifier, the backtick character `` can be wrapped around the keyword. error => `error` Schema Imports and References to Named Schemas \u00b6 When authoring schemas, files may be imported using the import directive as follows: import idl \"foo.avdl\" to import an IDL schema import schema \"foo.avsc\" to import a JSON schema Imported file names are resolved relative to the current IDL file References to Named Schemas and Types If a named type has already been defined or imported in the same Avro IDL file, it may be referenced by name, as if it were a primitive type. Example: Given a schema specifying a suit of cards: enum Suit { SPADES , HEARTS , DIAMONDS , CLUBS } It can be referenced in a schema specifying a card as: record Card { Suit suit ; // refers to the enum Card defined above int number ; } That's it! You now have all the information needed to author schemas using the Avro IDL . Take a glance at the usage section if you're not sure where to begin. References \u00b6 Apache Avro Documentation Avro Specification Avro IDL Need help? Post your question on these Slack channels: Dev Go Talk @ #dev-go-talk Did you find this page useful? Yes! No Maintained by: Kelvin Spencer","title":"Avro IDL"},{"location":"authoring-schemas-with-avro-idl/#usage","text":"In this section, we will walk through the creation of an event schema using the Avro IDL , generate the corresponding Avro JSON schema which can then be used by the avrogo tool to generate Go data structures for your services. This should provide the necessary context before diving into the specifics of authoring schemas with the Avro IDL . Avro IDL Schema declaration First, make sure you have a version of java (such as OpenJDK ) installed on your machine Download and install the avro-tools Check that the avro-tools have been properly installed by running java -jar avro-tools-{$version}.jar where {$version} is the version of the avro-tools you just downloaded Example: Schema declaration for a service Schema with imported headers @ namespace ( \"service_name\" ) protocol service_event { /* headers.avdl contains the standard headers that must be included with every avro event. */ import idl \"../headers.avdl\" ; /* add other schema imports if needed import idl \"my_other_service.avdl\"; import schema \"my_other_service.avsc\" */ @ meta ({ \"commentary\" : \"This Schema describes version v0.0.0 of the event service_event from the service_name.\" , \"version\" : \"0.0.0\" , \"event\" : \"service_event\" , \"eventDescription\" : \"TODO: Please describe the service_event here.\" , \"eventTriggeredBy\" : \"TODO: Please desrcibe the circumstances under which the service_event event is triggered, here.\" , \"status\" : \"active\" , \"partitions\" : \"1\" , \"topickey\" : \"service_name.service_event\" }) /* Put your enum definitions here. */ record service_event { /* Do not remove the following line */ meta Metadata ; /* Put your payload definition here. */ } Add service-specific data to the schema template Update schema template with service data @ namespace ( \"service_name\" ) protocol service_event { /* headers.avdl contains the standard headers that must be included with every avro event. */ import idl \"headers.avdl\" ; @ meta ({ \"commentary\" : \"This Schema describes version v0.0.0 of the event service_event from the service_name.\" , \"version\" : \"0.0.0\" , \"event\" : \"service_event\" , \"eventDescription\" : \"TODO: Please describe the service_event event here.\" , \"eventTriggeredBy\" : \"TODO: Please desrcibe the circumstances under which the service_event event is triggered, here.\" , \"status\" : \"active\" , \"partitions\" : \"1\" , \"topickey\" : \"service_name.service_event\" }) /* Put your enum definitions here. */ enum Type { payment , payment_adjustment , direct_payment } enum Mode { cash , card , in_car_with_fee , paid_by_platform } record service_event { /* Do not remove the following line */ meta Metadata ; /* Put your payload definition here. */ Type PaymentType ; Mode PaymentMode ; string ID ; union { null , string } TransactionID ; // optional string RideID ; string BusinessEventName ; decimal ( 4 , 2 ) ChargedAmount ; decimal ( 4 , 2 ) PlatformFee ; decimal ( 3 , 2 ) DiscountAmount ; string Currency ; string Country ; union { null , string } Reason ; // optional @ TimeStamp ( \"timestamp-micros\" ) // using a custom annotation to specify timestamp-micros long CreatedAt ; timestamp_ms UpdatedAt ; // using the built-in Avro IDL timestamp_ms logicalType string ` error ` ; // using an escaped Avro IDL keyword } Generate Avro Schema file (`.avsc`) files from the `.avdl` file using the `idl2schemata` command java -jar avro-tools-{$version}.jar idl2schemata example_schema.avdl The new Avro Schema file ( .avsc ) will be generated in the same folder as the .avdl file Bonus You can also generate Go data structures from the .avsc file using the avrogo command","title":"Usage"},{"location":"authoring-schemas-with-avro-idl/#data-types","text":"As with programming languages, the AVRO IDL defines data types that can be used as building blocks for authoring Avro schemas. The AVRO IDL defines: primitive data types complex data types logical types Reader's Schema vs Writer's Schema Because a schema is used to tell the datatype of each field, binary data can only be decoded correctly if the code reading the data knows the exact schema that the data was written with. If it uses a different schema, arbitrary data corruption could result. When an application wants to encode data, it encodes the data using whatever schema version it has. This is known as the writer's schema . A header is added to the data that encodes a schema id for the schema, which is found by querying the schema registry. This means that an application producing Avro messages must be using a schema that is already registered. When an application wants to decode data, it reads it into its own schema, known as the reader's schema . This does not have to be the same as the writer's schema, but it does have to be compatible . The process of transforming from the writer's schema to the reader's schema is known as schema resolution and full details are available here . The key idea with Avro is that the writer's schema and the reader's schema do not have to be the same, they only need to be compatible.","title":"Data Types"},{"location":"authoring-schemas-with-avro-idl/#primitive-data-types","text":"In Avro , Primitive types have no specified attributes, and their names may not be defined in any namespace. As such, primitive types automatically qualify as defined type names. Avro IDL declaration for `string` data type string ID ; Equivalent Avro JSON schema (`.avsc`) for `string` data type { \"name\" : \"ID\" , \"type\" : \"string\" } The Avro IDL supports the following primitive types: Avro IDL declaration Equivalent Avro JSON schema Type Description null null no value boolean boolean binary value int int 32-bit signed integer long long 64-bit signed integer float float single precision (32-bit) IEEE 754 floating-point number double double double precision (64-bit) IEEE 754 floating-point number bytes bytes sequence of 8-bit unsigned bytes string string unicode character sequence","title":"Primitive Data Types"},{"location":"authoring-schemas-with-avro-idl/#complex-data-types","text":"Avro supports the following complex types: records enums arrays maps unions fixed","title":"Complex Data Types"},{"location":"authoring-schemas-with-avro-idl/#records","text":"Record Types Records are like structs in the sense that they can be composed of both primitive data types and complex data types. Avro IDL declaration for a record type record Service { string service_name ; boolean is_active ; timestamp - millis created_at ; }; Equivalent Avro JSON schema { \"type\" : \"record\" , \"name\" : \"Service\" , \"fields\" : [ { \"name\" : \"service_name\" , \"type\" : \"string\" }, { \"name\" : \"is_active\" , \"type\" : \"boolean\" } { \"name\" : \"created_at\" , \"type\" : \"long\" , \"logicalType\" : \"timestamp-millis\" } ] } Not all complex types can be embedded in a Record Though the Avro specifications suggest that complex types such as enum types and even record types can be embedded within a record , the Avro IDL does not currently support these types of embeddings. With the current implementation, record types can only contain the following types: array boolean double float int long map bytes string null union date time_ms timestamp_ms","title":"Records"},{"location":"authoring-schemas-with-avro-idl/#enums","text":"Enum Types Enumerated types are declared using the enum keyword. For example, to declare an enum describing a suit of playing cards: Avro IDL declaration for an enum type enum Suit { SPADES , HEARTS , DIAMONDS , CLUBS } Equivalent Avro JSON schema { \"type\" : \"enum\" , \"name\" : \"Suit\" , \"symbols\" : [ \"SPADES\" , \"HEARTS\" , \"DIAMONDS\" , \"CLUBS\" ] }","title":"Enums"},{"location":"authoring-schemas-with-avro-idl/#arrays","text":"Array Types The array type can be used to declare a list of items defined by the same attribute. Example: array of strings Avro IDL declaration for array of strings array < string > mountain_ranges ; Equivalent Avro JSON schema { \"type\" : \"array\" , \"name\" : \"mountain_ranges\" , \"items\" : \"string\" } Example: array of ints Avro IDL declaration for array of ints array < int > number_bases ; Equivalent Avro JSON schema { \"type\" : \"array\" , \"name\" : \"number_bases\" , \"items\" : \"int\" }","title":"Arrays"},{"location":"authoring-schemas-with-avro-idl/#maps","text":"Map Types The map type can be used to declare a list of key-value pairs, where all keys are constrained to be string types. Example: Map with strings as keys (by default), and longs as values Avro IDL declaration for a map with `longs` as values map < long > astronomical_units_to_meters ; Equivalent Avro JSON schema { \"type\" : \"map\" , \"name\" : \"astronomical_units_to_meters\" , \"values\" : \"long\" }","title":"Maps"},{"location":"authoring-schemas-with-avro-idl/#unions","text":"Union Types union types are often used to declare fields with optional values Example: union declaring a schema which may be either a null type or a string type Avro IDL declaration for an optional string union { null , string } optionalString ; Equivalent Avro JSON schema { \"name\" : \"optionalString\" , [ \"null\" , \"string\" ] } Rules When a default value is specified for a record whose type is a union , the type of the default value must match the first element of the union . Thus for unions containing null , the null SHOULD be listed first since the default value of such unions is typically null Restrictions on Combining unions unions may not immediately contain other unions unions may not contain more than one schema with the same type except for record types, fixed types and enum types unions containing two array types or two map types are not permitted, but two types with different names are permitted. This is because names permit efficient resolution when reading and writing unions .","title":"Unions"},{"location":"authoring-schemas-with-avro-idl/#fixed","text":"Fixed Types fixed types are user-defined types where the specific number of bytes required to hold the type is explicitly declared. The number of bytes is specified as a positive integer. Example: a 16-byte quantity for holding an md5 hash Avro IDL declaration for an MD5 hash type fixed MD5 ( 16 ); Equivalent Avro JSON schema { \"type\" : \"fixed\" , \"size\" : 16 , \"name\" : \"MD5\" }","title":"Fixed"},{"location":"authoring-schemas-with-avro-idl/#logical-types","text":"A logical type is an Avro primitive or complex type with extra attributes to represent a derived type. The attribute logicalType must always be present for a logical type. It is a string representation of one of these logical types: decimal uuid date time-micros timestamp-micros time-millis timestamp-millis Only timestamp-millis and time-millis and date supported In the current implementation of the Avro IDL , only the timestamp-millis , time-millis and date logicalType are supported. The other logicalTypes are supported in Avro JSON schemas, but cannot be generated with the Avro IDL . Serialization of logicalType A logical type is always serialized using its underlying Avro type so that values are encoded in exactly the same way as the equivalent Avro type that does not have a logicalType attribute. Thus, if a logical type is invalid, for example a decimal with scale greater than its precision, then implementations COULD ignore the logical type and use the underlying Avro type. Decimal Logical Type Not Yet supported in Avro IDL The decimal logical type represents an arbitrary-precision signed decimal number of the form: unscaled \u00d7 10-scale. Example: schema representing decimal numbers with a maximum precision of 4 and a scale of 2 Avro IDL declaration for the decimal logical type decimal ( 4 , 2 ) dailyEarnings ; Equivalent Avro JSON schema { \"type\" : \"bytes\" , // underlying Avro type \"logicalType\" : \"decimal\" , \"name\" : \"dailyEarnings\" , \"precision\" : 4 , \"scale\" : 2 } Attributes Required or Optional? Description precision REQUIRED the MAXIMUM number of digits that can be represented, irrespective of the position of the decimal point scale OPTIONAL the number of fractional digits. If not specified, the scale is defined as 0 Ex: Given the declaration: decimal(4) dailyEarnings; The following decimals are all valid: 9999 -9999 ... and all other digits between {-9999, 9999} A scale value of 0 resolves to integer values since there are no fractional digits Restrictions precision must be a positive integer (greater than zero). If the underlying type is a fixed type, then the precision is limited by its size scale must be 0 or a positive integer less than or equal to the precision Ex: Given the declaration: decimal(4, 2) rideFees; The following decimals are all valid: 99.99 -99.99 ... and all other decimals between {-99.99, 99.99} A scale value equal to the precision value resolves to a decimal with only fractional digits Ex: Given the declaration: decimal (4, 4) commission; The following decimals are valid: 0.999 -0.999 ... and all other decimals between {-0.999, 0.999} When resolving schemas, two schemas that are decimal logical types match if their scales and precisions match UUID Logical Type Not Yet supported in Avro IDL The uuid logical type represents a random generated, universally unique identifier (UUID). It annotates an Avro string type that conforms with RFC-4122 Example: named schema for representing a user_id with a UUID logical type Avro IDL declaration for a UUID logical type UUID user_id ; Equivalent Avro JSON schema { \"name\" : \"user_id\" , \"type\" : \"string\" , \"logicalType\" : \"UUID\" } Date Logical Type Not Yet supported in Avro IDL The date logical type represents a date, with NO reference to time zone or time of day. It annotates an Avro int type, where the int stores the number of days from the unix epoch (1 January 1970 [ISO Calendar]) Example: named schema for representing April Fool's day with a date logical type Avro IDL declaration for a date logical type date april_fools_day ; Equivalent Avro JSON schema { \"name\" : \"april_fools_day\" , \"type\" : \"int\" , \"logicalType\" : \"date\" } Logical Types for representing Time Logical Type Precision Underlying Avro Type Storage Description time-millis 1 millisecond int number of milliseconds after midnight, 00:00:00.000 represents a time of day, with no reference to a particular calendar, time zone or date time-micros 1 microsecond long numer of microseconds after midnight, 00:00:00.000000 represents a time of day, with no reference to a particular calendar, time zone or date timestamp-millis 1 millisecond long number of milliseconds from the unix epoch, 1 January 1970 00:00:00.000 UTC represents an instant on the global timeline, independent of a particular time zone or calendar timestamp-micros 1 microsecond long number of microseconds from the unix epoch, 1 January 1970 00:00:00.000000 UTC represents an instant on the global timeline, independent of a particular time zone or calendar Example: named timestamp for representing the time a user account was created, using a timestamp-millis logical type Avro IDL declaration for a timestamp-millis logical type timestamp - millis created_at ; Equivalent Avro JSON schema { \"name\" : \"created_at\" , \"type\" : \"long\" , \"logicalType\" : \"timestamp-millis\" } timestamp-micros and time-micros not currently supported in Avro-IDL. Use a custom annotation instead such as: @ serviceTimeStamp ( \"timestamp-micros\" ) // using a custom annotation to specify timestamp-micros long CreatedAt ;","title":"Logical Types"},{"location":"authoring-schemas-with-avro-idl/#naming-conventions","text":"","title":"Naming Conventions"},{"location":"authoring-schemas-with-avro-idl/#named-types","text":"Defining named types records , enums , and fixed types are named types, where each fullname consists of: name : starts with characters that match [A-Za-z_] , and CAN subsequently contain characters that match [A-Za-z0-9_] namespace : dot-separated sequence of name -like tokens that qualify a name . The empty string can be used to indicate the null namespace Restrictions The uniqueness of the name associated with a named type is defined on the fullname The check for equality on names (including field names and enum symbols) is case-sensitive A schema may not contain multiple definitions of a fullname A name must be defined before it is used","title":"Named Types"},{"location":"authoring-schemas-with-avro-idl/#aliases","text":"Using aliases for named types and fields Aliases provide a mechanism for flexibility when declaring a schema. They allow named types and fields to be optionally mapped to other named types and fields, which makes it easier for schemas to evolve. Example: specifying type and field aliases, using the @alias annotation @ aliases ([ \"OldRecord\" , \"AncientRecord\" ]) record MyRecord { // map the named record type `MyRecord` to `OldRecord` and `AncientRecord` string @ aliases ([ \"oldField\" , \"ancientField\" ]) myNewField ; // map the record's string field `myNewField` to `oldField` and `ancientField` } Consequently, the named record type MyRecord can also be referenced as OldRecord or AncientRecord . Similarly, MyRecord 's named string field myNewField can also be referenced as oldField or ancientField . In the context of event driven applications, this means that producers can safely update the schemas, without worrying about introducing breaking changes for the consumers , since aliases provide a mechanism for backwards compatibility.","title":"Aliases"},{"location":"authoring-schemas-with-avro-idl/#annotations-comments-and-other-language-constructs","text":"The annotation tag ( @ ) is used to specify certain properties of the annotated element. Here are some common annotations: annotation description @namespace specify namespace of a record , or modify the namespace when defininig a named schema @aliases specify type and field aliases of a record Examples of annotations Modify the namespace when defining a named schema, using the @namespace annotation @ namespace ( \"legacy.bookkeeping\" ) protocol transaction_created { @ namespace ( \"current.bookkeeping\" ) record current_transaction {} record legacy_transaction {} } where the transaction_created protocol is defined in the legacy.bookkeeping namespace, the record current_transaction is defined in the current.bookkeeping namespace and the record legacy_transaction is defined in the legacy.bookkeeping namespace, since it inherits its default namespace from its container. annotations can be used to include arbitrary JSON elements JSON elements that are not directly supported in the Avro IDL , such as service-specific metadata, or certain logical types such as timestamp-micros can be declared using annotation tags. For example: Declaring a service-specific timestamp using annotations @ serviceTimeStamp ( \"timestamp-micros\" ) // using the generic annotation to specify timestamp-micros long CreatedAt ; Equivalent Avro JSON format { \"name\" : \"CreatedAt\" , \"type\" : { \"type\" : \"long\" , \"serviceTimeStamp\" : \"timestamp-micros\" } } Comments The Avro IDL supports C -style comments, where: any text following // on a line is ignored any text between /* and */ (possibly spanning multiple lines) is ignored Using reserved language keywords as identifiers In the rare occasion that a reserved language keyword needs to be used as an identifier, the backtick character `` can be wrapped around the keyword. error => `error`","title":"Annotations, Comments and Other Language Constructs"},{"location":"authoring-schemas-with-avro-idl/#schema-imports-and-references-to-named-schemas","text":"When authoring schemas, files may be imported using the import directive as follows: import idl \"foo.avdl\" to import an IDL schema import schema \"foo.avsc\" to import a JSON schema Imported file names are resolved relative to the current IDL file References to Named Schemas and Types If a named type has already been defined or imported in the same Avro IDL file, it may be referenced by name, as if it were a primitive type. Example: Given a schema specifying a suit of cards: enum Suit { SPADES , HEARTS , DIAMONDS , CLUBS } It can be referenced in a schema specifying a card as: record Card { Suit suit ; // refers to the enum Card defined above int number ; } That's it! You now have all the information needed to author schemas using the Avro IDL . Take a glance at the usage section if you're not sure where to begin.","title":"Schema Imports and References to Named Schemas"},{"location":"authoring-schemas-with-avro-idl/#references","text":"Apache Avro Documentation Avro Specification Avro IDL Need help? Post your question on these Slack channels: Dev Go Talk @ #dev-go-talk Did you find this page useful? Yes! No Maintained by: Kelvin Spencer","title":"References"},{"location":"getting-started-with-cuelang/","text":"Cue is a general-purpose, strongly-typed constraint-based language. In Cue, there is no distinction between types and values . Specifically, types are values . Further, values are organized around the concept of a value lattice . Content values ordering constraints enforcing constraints practice with constraints working with structs working with references authoring Cue schemas The Value Lattice \u00b6 A value lattice defines the hierarchy of values or types . By combining the relationship of values in the value lattice , with defined precedence rules on a set of constraint operators, Cue encourages simple constructs without sacrificing formal rigor. The value lattice is a partially ordered set, in which every two elements have a unique least upper bound, and a greatest lower bound. For every two elements, there is a unique instance of both elements that subsumes all other elements that are an instance of both elements. For example 42.0 is an instance of a float , and 7 is an instance of an int . Both 42.0 and 7 are instances of a number The value lattice allows for constraints to be enforced, based on the relationship of values within the lattice. At the top of the value lattice is a single ancestor of all values, called top . It is denoted by _ . Every value in the lattice is an instance of top . At the bottom of the value lattice is a single leaf, called bottom . It is denoted by _|_ . A bottom value usually indicates an error. Values \u00b6 Beyond simple values such as 7 , 42.0 , \"hello\" , Cue provides structs for building complex values. In Cue, structs are simply a mapping of labels to values. It is a set of elements (called fields ), each of which has a name (called label ), and a value The unique struct , with no field, {} is an instance of every struct. The fields in a struct may be required or optional. Example Cue Struct { a: 7, b:42.0, c: \"hello\" } Atomic Values Within the value lattice , an atom is any value whose only instances are itself and bottom , such as null , true , 42.0 Concrete Values In Cue, a value is concrete if it is either an atom , or a struct whose field values recursively evaluate to concrete values. Null Value The null value is represented with the keyword null . It has only one parent top , and one child bottom . Boolean Value The boolean value represents the set of Boolean truth values denoted by the keywords true and false . Numeric Values int and float values are distinct instances of the generic number value in Cue. int represents the set of all integer numbers. float represents the set of all decimal floating-point numbers. Ordering Constraints \u00b6 While the value lattice provides a hierarchy for defining the relationship between values , the precedence rules on constraint operators define the order in which constraints are applied to values within the value lattice . Order of constraint operators in increasing precedence | => alternation () => grouping [] => option (0 or once) {} => repetition (0 to n times) Enforcing Constraints \u00b6 Given a value lattice which defines the relationship between values and a set of precedence rules for constraint operators, Cue employs two mechanisms to enforce constraints. Unification \u00b6 Unification The unification of values a and b is defined as the greatest lower bound of a and b within the value lattice . Since Cue values form a lattice, the unification of two Cue values is always unique: a & a = a a & b = a //where a is an instance of b a & _|_ = _|_ //the unification of any value with bottom is always bottom Unifications are: commutative 1 (changing the order of the operands does not change the results) associative 2 (the order in which the operations are performed does not matter as long as the sequence of the operands is not changed). Thus, the order of evaluation is irrelevant. Disjunction \u00b6 Disjunction The disjunction of values a and b is defined as the least upper bound of a and b within the value lattice . Since Cue values form a lattice, the disjunction of two Cue values is always unique: a | a = a a | b = b //where a is an instance of b a | _|_ = a //the disjunction of any value with bottom is always that value _|_ | _|_ = _|_ //the disjunction of two bottom values is bottom Disjunctions are: commutative 1 (changing the order of the operands does not change the results) associative 2 (the order in which the operations are performed does not matter as long as the sequence of the operands is not changed) idempotent 3 (can be applied multiple times without changing the result beyond the initial application) A disjunction is normalized if there is no element a for which there is an element b such that a is an instance of b Applying Constraints to Values \u00b6 Remember that the value lattice is a partially ordered set, in which every two elements have a unique least upper bound , and a greatest lower bound . For every two elements there is a unique instance of both elements that subsumes all other elements that are an instance of both elements Unification of Values with top \u00b6 Unifying any value v with top _ results in v itself Expression _ & 5 _ & _ _ & _|_ Result 5 _ _|_ Disjunctions with default values \u00b6 Any element of a disjunction can be marked as a default by prefixing it with an asterisk * . Consequently, when an expression needs to be resolved for an operation other than unification or disjunction, non-starred elements are dropped in favor of starred ones, if the starred elements do not resolve to bottom . Expression \"tcp\" | \"udp\" * \"tcp\" | \"udp\" float | * 1 *string | 1.0 Result \"tcp\" | \"udp\" \"tcp\" 1 string Combining unifications and disjunctions with the operator precedence rules \u00b6 Order of constraint operators in increasing precedence | => alternation () => grouping [] => option (0 or once) {} => repetition (0 to n times) Expression ({a:1} | {b:2}) & {c:3} Result {a:1, c:3} | {b:2, c:3} Unification is commutative 1 and associative 2 Expression (int | string) & \"foo\" Result \"foo\" // greatest lower bound Expression \"tcp\" | \"udp\" Result \"tcp\" | \"udp\" // disjunction with no default values Expression *\"tcp\" | \"udp\" (*\"tcp\"|\"udp\") & (\"udp\"|*\"tcp\" (*\"tcp\"|\"udp\") & (\"udp\"|\"tcp\") (*\"tcp\"|\"udp\") & \"tcp\" Result \"tcp\" // default value and greatest lower bound Expression (*true | false) & bool Result true // default value and greatest lower bound Working with Structs \u00b6 Because a struct is a set of elements ( fields ), each of which has a name ( label ), and a value, operations on structs are analogous to basic set operations. The unique struct ( {} ), with no field is an instance of every struct, just as the empty set is an instance of every set. Further, the fields in a struct may be required or optional. Given a value lattice , a set of ordering constraints and the list of comparison operators below: == equal != not equal < less <= less or equal > greater >= greater or equal =~ matches regular expression !~ does not match regular expression we can succinctly define the constraints to apply to set of values. Expression {a: >=1 & <=7} & {a: >=5 & <=9} which represents the concrete set operation {1,2,3,4,5,6,7} & {5,6,7,8,9} Result {a: >=5 & <=7} which represents the concrete set {5,6,7} Expression {a: 1} & {a: 2} Result _|_ which represents the greatest lower bound from unification For structs with optional fields, optional labels are defined with an expression to select all labels for which a given constraint should be applied. For example: [\"foo\"]: bar where the square brackets holds a set of expressions to indicate the matching labels of an optional field In Cue, we use the syntactic shorthand ? to indicate optional labels foo?: bar Declaring struct fields struct fields can be declared as either: regular (using : ) or definition (using :: ) Whereas regular fields are required to be concrete when emitting data, definitions are not required to be concrete since they are not emitted as part of the model. Example: struct with optional field Given the definition: nameMap: [ string ] : { // optional field label firstName: string nickName: *firstName | string // default to the resolved firstName } nameMap: hank: { firstName: \"Hank\" } nameMap defines an optional field set [string]: { ... } , which in this specific case is just hank , and unifies the associated constraint with the matched field resolves to: nameMap: hank: { firstName: \"Hank\" nickName : \"Hank\" } structs are open by default Because structs are open to adding fields, instances of one struct may contain fields that are not defined in the struct. This makes it easy to add fields, but could also lead to bugs. How do we resolve this? We can close the struct by adding an optional field with value _|_ for all undefined fields, or by defining the struct with the close keyword A: close( { field1: string field2: string } ) This makes it easy to declare instances of the struct where the fields can be validated (including spelling errors!) A 1 : A & { feild1: string } // _|_ feild 1 not defined for A Working with nested structs structs may contain fields whose values are also structs . Cue provides a syntactic shorthand for defining structs with a field whose value is a struct with a single field . This may be written as a colon-separated sequence of the two field names, followed by a colon and the value of that single field . normally defined nested struct job: { myTask: { replicas: 2 } } syntactic shorthand job: myTask: replicas: 2 Working with References \u00b6 Cue provides a mechanism for referencing fields in expressions such as: struct with a referenced field a: { place: string greeting: \"Hello, \\(place)!\" // the referenced _field_ `place` interpolated with the _value_ expression for the `greeting` _field_ } unification with a concrete value b: a & { place: \"world\" } c: a & { place: \"you\" } evaluation after unification d: b.greeting // \"Hello, world!\" e: c.greeting // \"Hello, you!\" Authoring Cue schemas \u00b6 Though we have barely covered a fraction of Cue and its capabilities, we now know enough to author a simple, but practical Cue schema. If the idea of playing around with Cue seems more appealing than authoring a schema, then check out the [Cue Language Tutorials] (https://cuelang.org/docs/tutorials/) Example Service Deployment Schema service: [ ID=_ ] : { // the service name is derived from the template `[ID=_]` which matches any _value_ since every _value_ in the _value lattice_ is an instance of _top_ (`_`) apiVersion: \"v1\" kind : \"Service\" metadata : { name: ID // reference the value of `ID` labels: { app: ID // reference the value of `ID` by convention domain: \"prod\" // always the same in the given files component : string // varies per directory // the field of a name component must be set to some string value } } spec: { // Any port has the following properties. ports: [...{ // the ellipsis defines `ports` as an open struct since it defines _top_ (`_`) for all labels. This means we can have 1 or more ports defined here port: int protocol: * \"TCP\" | \"UDP\" // use the `TCP` protocol by default if a protocol is not defined name : string | * \"client\" // use `client` as the default name for the port is a port name is not defined } ] } } Example Service Test Schema package roundtrip import ( avroPkg \"github.com/heetch/cue-schema/avro\" ) tests: [ _ ] : roundTripTest // roundTripTest is an instance of a service test tests: [ name=_ ] : testName: name // the test name is derived from the template ` [ name=_ ] ` which matches any _value_ since every _value_ in the _value lattice_ is an instance of _top_ (`_`) roundTripTest :: { // define the roudTripTest testName: string inSchema: avro.Schema // avro schema defined in the avroPkg outSchema?: avro.Schema // optional output schema extraSchemas?: [... avro.Schema] // optional field, with 1 or more schemas goType: *outSchema.name | string // default to the value of the `name` field defined in outSchema goTypeBody?: string // optional // generateError holds the error expected from invoking avrogo. // If this is specified, there will be no generated test package. generateError?: string // optional inData?: _ // optional, matches any _value_ outData?: _ // optional, matches any _value_ expectError?: [errorKind]: string // optional, reference to the declaration of `errorKind` below, constrained to a `string` value otherTests?: string // optional, constrained to a `string` value subtests: [name=_]: { //the name of subtests derived from the template `[name=_]` which matches any _value_ since every _value_ in the _value lattice_ is an instance of _top_ (`_`) testName: name inData: _ // matches any _value_ outData: _ // matches any _value_ expectError?: [errorKind]: string // optional, reference to the declaration of `errorKind` below, constrained to a `string` value } if inData != _|_ { // ensure the input data is valid (not an error value) subtests: main: { \"inData\" : inData \"outData\" : outData if expectError != _|_ { \"expectError\" : expectError } } } } errorKind :: \"unmarshal\" | \"marshal\" Reference \u00b6 Cue Language Specification Cue Language Tutorials Need help? Post your question on these Slack channels: Dev Go Talk @ #dev-go-talk Did you find this page useful? Yes! No Maintained by: Kelvin Spencer commutative : a x b = b x a ; a + b = b + a \u21a9 \u21a9 \u21a9 associative : ( a + b ) + c = a + ( b + c ) ; ( a x b ) x c = a x ( b x c ) \u21a9 \u21a9 \u21a9 idempotent : a | a = a ; b | b | b = b \u21a9","title":"Cuelang"},{"location":"getting-started-with-cuelang/#the-value-lattice","text":"A value lattice defines the hierarchy of values or types . By combining the relationship of values in the value lattice , with defined precedence rules on a set of constraint operators, Cue encourages simple constructs without sacrificing formal rigor. The value lattice is a partially ordered set, in which every two elements have a unique least upper bound, and a greatest lower bound. For every two elements, there is a unique instance of both elements that subsumes all other elements that are an instance of both elements. For example 42.0 is an instance of a float , and 7 is an instance of an int . Both 42.0 and 7 are instances of a number The value lattice allows for constraints to be enforced, based on the relationship of values within the lattice. At the top of the value lattice is a single ancestor of all values, called top . It is denoted by _ . Every value in the lattice is an instance of top . At the bottom of the value lattice is a single leaf, called bottom . It is denoted by _|_ . A bottom value usually indicates an error.","title":"The Value Lattice"},{"location":"getting-started-with-cuelang/#values","text":"Beyond simple values such as 7 , 42.0 , \"hello\" , Cue provides structs for building complex values. In Cue, structs are simply a mapping of labels to values. It is a set of elements (called fields ), each of which has a name (called label ), and a value The unique struct , with no field, {} is an instance of every struct. The fields in a struct may be required or optional. Example Cue Struct { a: 7, b:42.0, c: \"hello\" } Atomic Values Within the value lattice , an atom is any value whose only instances are itself and bottom , such as null , true , 42.0 Concrete Values In Cue, a value is concrete if it is either an atom , or a struct whose field values recursively evaluate to concrete values. Null Value The null value is represented with the keyword null . It has only one parent top , and one child bottom . Boolean Value The boolean value represents the set of Boolean truth values denoted by the keywords true and false . Numeric Values int and float values are distinct instances of the generic number value in Cue. int represents the set of all integer numbers. float represents the set of all decimal floating-point numbers.","title":"Values"},{"location":"getting-started-with-cuelang/#ordering-constraints","text":"While the value lattice provides a hierarchy for defining the relationship between values , the precedence rules on constraint operators define the order in which constraints are applied to values within the value lattice . Order of constraint operators in increasing precedence | => alternation () => grouping [] => option (0 or once) {} => repetition (0 to n times)","title":"Ordering Constraints"},{"location":"getting-started-with-cuelang/#enforcing-constraints","text":"Given a value lattice which defines the relationship between values and a set of precedence rules for constraint operators, Cue employs two mechanisms to enforce constraints.","title":"Enforcing Constraints"},{"location":"getting-started-with-cuelang/#unification","text":"Unification The unification of values a and b is defined as the greatest lower bound of a and b within the value lattice . Since Cue values form a lattice, the unification of two Cue values is always unique: a & a = a a & b = a //where a is an instance of b a & _|_ = _|_ //the unification of any value with bottom is always bottom Unifications are: commutative 1 (changing the order of the operands does not change the results) associative 2 (the order in which the operations are performed does not matter as long as the sequence of the operands is not changed). Thus, the order of evaluation is irrelevant.","title":"Unification"},{"location":"getting-started-with-cuelang/#disjunction","text":"Disjunction The disjunction of values a and b is defined as the least upper bound of a and b within the value lattice . Since Cue values form a lattice, the disjunction of two Cue values is always unique: a | a = a a | b = b //where a is an instance of b a | _|_ = a //the disjunction of any value with bottom is always that value _|_ | _|_ = _|_ //the disjunction of two bottom values is bottom Disjunctions are: commutative 1 (changing the order of the operands does not change the results) associative 2 (the order in which the operations are performed does not matter as long as the sequence of the operands is not changed) idempotent 3 (can be applied multiple times without changing the result beyond the initial application) A disjunction is normalized if there is no element a for which there is an element b such that a is an instance of b","title":"Disjunction"},{"location":"getting-started-with-cuelang/#applying-constraints-to-values","text":"Remember that the value lattice is a partially ordered set, in which every two elements have a unique least upper bound , and a greatest lower bound . For every two elements there is a unique instance of both elements that subsumes all other elements that are an instance of both elements","title":"Applying Constraints to Values"},{"location":"getting-started-with-cuelang/#unification-of-values-with-top","text":"Unifying any value v with top _ results in v itself Expression _ & 5 _ & _ _ & _|_ Result 5 _ _|_","title":"Unification of Values with top"},{"location":"getting-started-with-cuelang/#disjunctions-with-default-values","text":"Any element of a disjunction can be marked as a default by prefixing it with an asterisk * . Consequently, when an expression needs to be resolved for an operation other than unification or disjunction, non-starred elements are dropped in favor of starred ones, if the starred elements do not resolve to bottom . Expression \"tcp\" | \"udp\" * \"tcp\" | \"udp\" float | * 1 *string | 1.0 Result \"tcp\" | \"udp\" \"tcp\" 1 string","title":"Disjunctions with default values"},{"location":"getting-started-with-cuelang/#combining-unifications-and-disjunctions-with-the-operator-precedence-rules","text":"Order of constraint operators in increasing precedence | => alternation () => grouping [] => option (0 or once) {} => repetition (0 to n times) Expression ({a:1} | {b:2}) & {c:3} Result {a:1, c:3} | {b:2, c:3} Unification is commutative 1 and associative 2 Expression (int | string) & \"foo\" Result \"foo\" // greatest lower bound Expression \"tcp\" | \"udp\" Result \"tcp\" | \"udp\" // disjunction with no default values Expression *\"tcp\" | \"udp\" (*\"tcp\"|\"udp\") & (\"udp\"|*\"tcp\" (*\"tcp\"|\"udp\") & (\"udp\"|\"tcp\") (*\"tcp\"|\"udp\") & \"tcp\" Result \"tcp\" // default value and greatest lower bound Expression (*true | false) & bool Result true // default value and greatest lower bound","title":"Combining unifications and disjunctions with the operator precedence rules"},{"location":"getting-started-with-cuelang/#working-with-structs","text":"Because a struct is a set of elements ( fields ), each of which has a name ( label ), and a value, operations on structs are analogous to basic set operations. The unique struct ( {} ), with no field is an instance of every struct, just as the empty set is an instance of every set. Further, the fields in a struct may be required or optional. Given a value lattice , a set of ordering constraints and the list of comparison operators below: == equal != not equal < less <= less or equal > greater >= greater or equal =~ matches regular expression !~ does not match regular expression we can succinctly define the constraints to apply to set of values. Expression {a: >=1 & <=7} & {a: >=5 & <=9} which represents the concrete set operation {1,2,3,4,5,6,7} & {5,6,7,8,9} Result {a: >=5 & <=7} which represents the concrete set {5,6,7} Expression {a: 1} & {a: 2} Result _|_ which represents the greatest lower bound from unification For structs with optional fields, optional labels are defined with an expression to select all labels for which a given constraint should be applied. For example: [\"foo\"]: bar where the square brackets holds a set of expressions to indicate the matching labels of an optional field In Cue, we use the syntactic shorthand ? to indicate optional labels foo?: bar Declaring struct fields struct fields can be declared as either: regular (using : ) or definition (using :: ) Whereas regular fields are required to be concrete when emitting data, definitions are not required to be concrete since they are not emitted as part of the model. Example: struct with optional field Given the definition: nameMap: [ string ] : { // optional field label firstName: string nickName: *firstName | string // default to the resolved firstName } nameMap: hank: { firstName: \"Hank\" } nameMap defines an optional field set [string]: { ... } , which in this specific case is just hank , and unifies the associated constraint with the matched field resolves to: nameMap: hank: { firstName: \"Hank\" nickName : \"Hank\" } structs are open by default Because structs are open to adding fields, instances of one struct may contain fields that are not defined in the struct. This makes it easy to add fields, but could also lead to bugs. How do we resolve this? We can close the struct by adding an optional field with value _|_ for all undefined fields, or by defining the struct with the close keyword A: close( { field1: string field2: string } ) This makes it easy to declare instances of the struct where the fields can be validated (including spelling errors!) A 1 : A & { feild1: string } // _|_ feild 1 not defined for A Working with nested structs structs may contain fields whose values are also structs . Cue provides a syntactic shorthand for defining structs with a field whose value is a struct with a single field . This may be written as a colon-separated sequence of the two field names, followed by a colon and the value of that single field . normally defined nested struct job: { myTask: { replicas: 2 } } syntactic shorthand job: myTask: replicas: 2","title":"Working with Structs"},{"location":"getting-started-with-cuelang/#working-with-references","text":"Cue provides a mechanism for referencing fields in expressions such as: struct with a referenced field a: { place: string greeting: \"Hello, \\(place)!\" // the referenced _field_ `place` interpolated with the _value_ expression for the `greeting` _field_ } unification with a concrete value b: a & { place: \"world\" } c: a & { place: \"you\" } evaluation after unification d: b.greeting // \"Hello, world!\" e: c.greeting // \"Hello, you!\"","title":"Working with References"},{"location":"getting-started-with-cuelang/#authoring-cue-schemas","text":"Though we have barely covered a fraction of Cue and its capabilities, we now know enough to author a simple, but practical Cue schema. If the idea of playing around with Cue seems more appealing than authoring a schema, then check out the [Cue Language Tutorials] (https://cuelang.org/docs/tutorials/) Example Service Deployment Schema service: [ ID=_ ] : { // the service name is derived from the template `[ID=_]` which matches any _value_ since every _value_ in the _value lattice_ is an instance of _top_ (`_`) apiVersion: \"v1\" kind : \"Service\" metadata : { name: ID // reference the value of `ID` labels: { app: ID // reference the value of `ID` by convention domain: \"prod\" // always the same in the given files component : string // varies per directory // the field of a name component must be set to some string value } } spec: { // Any port has the following properties. ports: [...{ // the ellipsis defines `ports` as an open struct since it defines _top_ (`_`) for all labels. This means we can have 1 or more ports defined here port: int protocol: * \"TCP\" | \"UDP\" // use the `TCP` protocol by default if a protocol is not defined name : string | * \"client\" // use `client` as the default name for the port is a port name is not defined } ] } } Example Service Test Schema package roundtrip import ( avroPkg \"github.com/heetch/cue-schema/avro\" ) tests: [ _ ] : roundTripTest // roundTripTest is an instance of a service test tests: [ name=_ ] : testName: name // the test name is derived from the template ` [ name=_ ] ` which matches any _value_ since every _value_ in the _value lattice_ is an instance of _top_ (`_`) roundTripTest :: { // define the roudTripTest testName: string inSchema: avro.Schema // avro schema defined in the avroPkg outSchema?: avro.Schema // optional output schema extraSchemas?: [... avro.Schema] // optional field, with 1 or more schemas goType: *outSchema.name | string // default to the value of the `name` field defined in outSchema goTypeBody?: string // optional // generateError holds the error expected from invoking avrogo. // If this is specified, there will be no generated test package. generateError?: string // optional inData?: _ // optional, matches any _value_ outData?: _ // optional, matches any _value_ expectError?: [errorKind]: string // optional, reference to the declaration of `errorKind` below, constrained to a `string` value otherTests?: string // optional, constrained to a `string` value subtests: [name=_]: { //the name of subtests derived from the template `[name=_]` which matches any _value_ since every _value_ in the _value lattice_ is an instance of _top_ (`_`) testName: name inData: _ // matches any _value_ outData: _ // matches any _value_ expectError?: [errorKind]: string // optional, reference to the declaration of `errorKind` below, constrained to a `string` value } if inData != _|_ { // ensure the input data is valid (not an error value) subtests: main: { \"inData\" : inData \"outData\" : outData if expectError != _|_ { \"expectError\" : expectError } } } } errorKind :: \"unmarshal\" | \"marshal\"","title":"Authoring Cue schemas"},{"location":"getting-started-with-cuelang/#reference","text":"Cue Language Specification Cue Language Tutorials Need help? Post your question on these Slack channels: Dev Go Talk @ #dev-go-talk Did you find this page useful? Yes! No Maintained by: Kelvin Spencer commutative : a x b = b x a ; a + b = b + a \u21a9 \u21a9 \u21a9 associative : ( a + b ) + c = a + ( b + c ) ; ( a x b ) x c = a x ( b x c ) \u21a9 \u21a9 \u21a9 idempotent : a | a = a ; b | b | b = b \u21a9","title":"Reference"},{"location":"using-go-modules/","text":"Introduction \u00b6 The aim of this guide is to provide just enough detail to get you productive when working with Go Modules . It is derived from a series of articles on Go Modules, written by Russ Cox , the creator of Go Modules. If you would like to delve deeper into the design considerations behind Go Modules, please refer to the Go Modules Proposal . If you would like to learn about all the commands, flags and options related to Go Modules, then the Go Documentation and the Go blog articles on Go Modules may be the right place to start. Spare me the lecture. I need to quickly learn how to: make the current directory the root of a module add a dependency set a dependency to a specific version(upgrade/downgrade) update all dependencies clean up unused dependencies set up the correct environment variables fix my failing builds What are Go Modules? \u00b6 In Go, programs are constructed by linking together packages. A package is built from one or more source files that together declare the constants , types , variables and functions that belong to it. These declarations are accessible by all files of the same package, and also exported for use in other packages. Go Modules introduce the concept of a module, defined as a collection of Go packages that are stored in a file tree, with a go.mod file at its root. This go.mod file defines the path for accessing the module (serving as its import path ) and lists the module's dependency requirements, which are the other modules needed to successfully build the module. Each dependency is written as a module path and a specific semantic version, in a semantic version string format. Concretely, a Go Module is a group of packages versioned as a single unit, with a go.mod file that declares the minimum requirements that must be satisfied by the module's dependencies, and where the group of packages share a common import path prefix. In addition to go.mod , Go Modules introduce a go.sum file, which contains the cryptographic hashes of the content of specific module versions. When Go downloads each module, it computes a hash of the file tree corresponding to that module. The hash and the version of the module are included in the binary. The Go command then uses the go.sum file to ensure that future downloads of a module retrieve the same bits as the first download, thus guaranteeing that the modules a project depends on, do not change unexpectedly. Additionally, Go Modules introduce an auditable checksum database which will be used by the go command to authenticate modules. Why Go Modules? \u00b6 Go Modules bring package versioning to Go. Versioning enables reproducible builds by ensuring that a program builds exactly the same way tomorrow as it does today. Modules also adopt semantic versioning, eliminates vendoring, and deprecates $GOPATH in favor of a project-based workflow. Previously, when go get needed a package, it always fetched the latest copy, delegating the download and update operations to version control systems like Git. This meant that without a concept of versioning, go get could not convey to users any expectations about what kind of changes to expect in a given update. With versioning, library writers and users can frame the expectation around updates. Consequently there is a clear expectation on breaking updates versus non-breaking updates. Why Semantic Import Versioning? \u00b6 Go Modules champion the Import Compatibility Rule , where the expectation is that newer versions of a package, with a given import path, should be backwards-compatible with older versions. If an old package and a new package have the same import path, then the new package must be backwards compatible with the old package. ex: \"github.com/foo/foo_package@v1.2.3\" => \"github.com/foo/foo_package@1.2.5\" If a breaking change is required, then a new package should be created with a new import path. ex: \"github.com/foo/foo_package@v1.2.3\" => \"github.com/foo/foo_package/v2@v2.0.0\" Because each major version has a different import path, a module can contain one of each major version of a package. This means modules need to specify only the minimum requirements for their dependencies. Further, since both v1 and v2 of a package can coexist, this makes it easy to upgrade the clients of a package one at a time while guaranteeing successful builds. v0 is special because of its special role in Semantic Versioning Precedence Rules where major versions >=1 have different import paths. How are the module versions to use in each build determined? \u00b6 Go Modules use the Minimal Version Selection (MVS) algorithm to determine which module versions to use in each build. A build of a module by itself uses the specific versions of required dependencies listed in the go.mod file. However, in larger builds, it only uses a newer version if a dependency in the build requires it. Minimal Version Selection always selects the minimal (oldest) module version that satisfies the overall requirements of a build. Thus, the release of a new version has no effect on the build. It assumes that each module declares its own dependency requirements (as a list of the minimum versions of other modules) It assumes that modules follow the Import Compatibility Rule where packages in newer versions should work as well as the older versions. As such, the dependency requirement guarantees only a minimum version, never a maximum version or a list of incompatible later versions. How does the Minimal Version Selection (MVS) algorithm construct the build list for a given module? \u00b6 To construct the build list for a given module, the MVS algorithm starts the list with the module itself, and then appends each dependency's own build list. If a module appears in the list multiple times, it keeps only the newest module version. The build list is constructed using a graph reachability algorithm, with the given module as the starting node. The algorithm recursively traverses the graph, taking care not to visit a node that has already been visited. Once the traversal is complete (once it has a rough build list of nodes, including nodes with multiple versions), it computes the final build list by keeping only the newest version of any listed module. All other versions in the rough build list are discarded. How does the MVS algorithm upgrade all modules to their latest version? \u00b6 To upgrade all modules to their latest versions, the MVS algorithm computes an upgraded build list by updating the module requirement graph so that every outbound link to any version of a module is replaced with one that links to the latest version of that module. It then uses the previous algorithm to determine the final build list. Commands: go get -u (without any arguments) => upgrade direct and indirect dependencies of the current package go get -u ./... (from the module root) => upgrade all direct and indirect dependencies of your module, excluding test dependencies go get -u=patch ./... (from the module root) => same as above, using the latest patch release go get -u -t ./... (from the module root) => upgrade all direct and indirect dependencies of your module, including test dependencies go get -u=patch -t ./... (from the module root) => same as above, using the latest patch release go get -u all => update all of the packages transitively imported by the main module (including test dependencies) go get -u github.com/foo => get the latest version of foo as well as the latest versions for all of the direct and indirect dependencies of foo How does the MVS algorithm upgrade one module to a specific newer version? \u00b6 To upgrade one module to a specific newer version, it first constructs the build list of the non-upgraded module as before. Then it appends the new module's build list. If a module appears in the list multiple times, it keeps only the newest module version. Commands: go get foo@v1.2.3 => get a specific version go get foo@latest => get the latest version go get foo@master => using a branch name such as @master obtains the latest commit regardless of whether or not it has a semantic versioning tag Common Workflow A common starting point when upgrading foo is: - `go get foo` or `go get foo@latest` and after things are working - `go get -u=patch foo` to upgrade all direct and indirect dependencies of `foo` How does the MVS algorithm downgrade one module to a specific older version? \u00b6 To downgrade one module to a specific older version, it rewinds the required version of each top-level dependency, until that dependency's build list no longer refers to newer versions of the downgraded module. Downgrading one module may require downgrading other modules, so the algorithm tries to downgrade as few other modules as possible. If a requirement is incompatible with the proposed downgrade (if the requirement's build list includes a now-disallowed module version), then it successively tries older versions until it finds one that is compatible with the downgrade. Commands: go get foo@v1.2.3 => downgrade the foo package to the specified version What advantage does this have over systems that use lock files? \u00b6 Because a module's dependency requirements are included with the module's source code to uniquely determine how to build it, and because the Minimal Version Selection algorithm provides high-fidelity builds by using the oldest version available that meets the dependency requirements, mechanisms such as lock files are not needed. In other systems, the lock file lists the specific versions a build should use. It only guarantees reproducible builds for whole programs, not for library modules. Why does using Go Modules make vendoring unnecessary? \u00b6 Previously, if you were using an external package and worried that it might change in unexpected ways, you copied it to your local repository, and stored the copy under a new import path that identified it as a local copy, for example: github.com/pkg => my_local_path/external/github.com/pkg Over time, this was encapsulated in tools that copied a dependency into your repository and also updated all the import paths wihin it to reflect the new location. However, these modifications made it harder to compare against, and even incorporate newer copies, required updates, etc., to other code using that dependency. Package managers like go dep evolved to become a tool that could be used for freezing external package dependencies. This introduced the concept of vendoring , where you could copy dependencies into the project without modifying the source files as long as the GOPATH was correctly set up. But these vendoring tools relied on metadata files such as glide.yml or dep.yml to provide reproducible builds, and could not help with deciding which versions of a package to use. Vendor directories: specify by their contents, the exact version of the dependencies to use when running go build . ensure the availability of those dependencies, even if the original copies disappear. are unwieldly and bloat the repositories in which they are used. GOPATH was used to: define the versions of dependencies. hold the source code for those dependencies. provide a way to infer the import path for code in a specific directory. With Go Modules, GOPATH s are no longer required because the go.mod file includes the full module path and defines the version of each dependency in use. Further, since a directory with a go.mod file marks the root of the directory tree, this directory serves as a self-contained workspace, and is separate from all other directories. Why are pseudo-versions used to identify untagged commits? \u00b6 To name untagged commits, Go Modules use the pseudo-version format v0.0.0-yyyymmddhhmmss-{commit identifier} to identify a specific commit made on a given date. The commit identifier is typically a shorted Git hash, and must have a commit time matching the UTC timestamp. The pseudo-version form is used so that Semantic Versioning Precedence Rules can be used to compare two pseudo-versions by commit time, since the timestamp encoding makes string comparison match time comparison. This also ensures that go mod always prefers a tagged semantic version(even if it's very old, such as v0.0.1 ), over an untagged pseudo-version, since it has a greater semantic version precedence than any v0.0.0 pre-release Can I use Go Modules along with the traditional GOPATH -based mechanisms? \u00b6 Currently, the Go command defaults to module mode when run in directories outside GOPATH/src that are marked by go.mod files in their roots. This can be overriden by setting the transitional environment variable $GO111MODULE to on or off . The default mode is auto . How can I make the current directory, the root of a module? \u00b6 go mod init : when working in a directory outside $GOPATH . This command writes a go.mod file at the root of your directory Can packages also have go.mod files? \u00b6 The go.mod file only appears in the root of the module. Packages in subdirectories have import paths consisting of the module path and the path to that subdirectory. What happens if a package is not provided by any module in go.mod ? \u00b6 The Go command automatically looks up the module containing that package and adds it to go.mod using the latest version. Latest is defined as the latest tagged stable version, or else, the latest tagged pre-release version, or else, the latest untagged version. Only direct dependencies are recorded in the go.mod file. Can modules be cached locally? \u00b6 Yes, they are cached locally in $GOPATH/pkg/mod How are different major versions of package distinguished? \u00b6 Every major version of a Go Module {v1, v2, v3, ...} uses a different module path. Starting at v2 , the module path must end in the major version. Semantic Import Versioning is used to give incompatible packages(packages with different major versions) different names. Can I build a program using different minor versions? \u00b6 No, since these are considered duplicates of a single module path. Go Modules allow a build to include at most one version of any particular module path, which means, at most, one of each major version. Allowing different major versions of a module (since they have different paths) enables incremental upgrades to a new major version. How do I clean up unused dependencies? \u00b6 go mod tidy What does go mod tidy do? \u00b6 go mod tidy finds all the packages transitively imported by packages in your module and adds new module requirements for packages not provided by any known modules, while removing requirements on modules that do not provide any imported packages. If a module provides packages that are only imported by projects that have not yet migrated to modules, it marks that module requirement with an // indirect comment. I get errors when I run go mod tidy , what could it be? \u00b6 When go mod tidy adds a requirement, it adds the latest version of the module. However, if your $GOPATH included an older version of a dependency that subsequently published a breaking change, go mod tidy will be unable to resolve the right package. Run go mod graph to get a graph of the module dependency requirements and then try downgrading to an older version with the go get command. How can I fix tests that are failing because they cannot write files in the package directory? \u00b6 Tests may fail when the package directory is in the module cache, which has read-only access. Configure your tests to copy the files they need to write to a temporary directory instead. How can I fix tests that are failing because they rely on relative paths to packages in another module \u00b6 Tests that use relative paths to locate and read files in another package can fail if the package resides in a different module. You can either: copy the test inputs into your module convert the test inputs from raw files to data embedded in .go source files How can I fix tests that are failing becase they expect go commands within the test to run in $GOPATH mode? \u00b6 Add a go.mod file to the source tree to be tested Set GO111MODULE=off How do I release a new module after committing my changes? \u00b6 tag your release git tag v1.2.3 publish your release with the same tag git push origin v1.2.3 The new go.mod file at the root of your module defines the canonical import path for the module and adds the new minimum version requirements How do I know which environment variable to use when working with Go Modules? \u00b6 GO111MODULE={on|off} => transitional variable to toggle the use of Go Modules GOPRIVATE => controls which modules the Go command considers to be private (not available publicly) and should therefore not use the proxy or checksum database. ex: export GOPRIVATE=github.com/foo GONOPROXY => overrides GOPRIVATE for the specific decision of whether to use a proxy. ex: export GONOPROXY=github.com/foo GONOSUMDB => overrides GOPRIVATE for the specific decision of whether to use the checksum database. ex: export GONOSUMDB=github.com/foo Help! My build is failing and I don't know where to start! \u00b6 Ensure that the following environment variables are appropriately set: GOPRIVATE , GONOPROXY , GONOSUMDB Run go mod tidy Run go mod graph to make sense of the module dependency requirements Apply your understanding of how dependency requirements are resolved based on the following scenarios: upgrading all modules upgrading one module downgrading one module If all else fails, someone in the #dev-go-talk may be able to help Reference \u00b6 original series of articles on Go Modules, written by Russ Cox , the creator of Go Modules Go blog articles on Go Modules Need help? Post your question on these Slack channels: Dev Go Talk @ #dev-go-talk Did you find this page useful? Yes! No Maintained by: Kelvin Spencer","title":"Go Modules"},{"location":"using-go-modules/#introduction","text":"The aim of this guide is to provide just enough detail to get you productive when working with Go Modules . It is derived from a series of articles on Go Modules, written by Russ Cox , the creator of Go Modules. If you would like to delve deeper into the design considerations behind Go Modules, please refer to the Go Modules Proposal . If you would like to learn about all the commands, flags and options related to Go Modules, then the Go Documentation and the Go blog articles on Go Modules may be the right place to start. Spare me the lecture. I need to quickly learn how to: make the current directory the root of a module add a dependency set a dependency to a specific version(upgrade/downgrade) update all dependencies clean up unused dependencies set up the correct environment variables fix my failing builds","title":"Introduction"},{"location":"using-go-modules/#what-are-go-modules","text":"In Go, programs are constructed by linking together packages. A package is built from one or more source files that together declare the constants , types , variables and functions that belong to it. These declarations are accessible by all files of the same package, and also exported for use in other packages. Go Modules introduce the concept of a module, defined as a collection of Go packages that are stored in a file tree, with a go.mod file at its root. This go.mod file defines the path for accessing the module (serving as its import path ) and lists the module's dependency requirements, which are the other modules needed to successfully build the module. Each dependency is written as a module path and a specific semantic version, in a semantic version string format. Concretely, a Go Module is a group of packages versioned as a single unit, with a go.mod file that declares the minimum requirements that must be satisfied by the module's dependencies, and where the group of packages share a common import path prefix. In addition to go.mod , Go Modules introduce a go.sum file, which contains the cryptographic hashes of the content of specific module versions. When Go downloads each module, it computes a hash of the file tree corresponding to that module. The hash and the version of the module are included in the binary. The Go command then uses the go.sum file to ensure that future downloads of a module retrieve the same bits as the first download, thus guaranteeing that the modules a project depends on, do not change unexpectedly. Additionally, Go Modules introduce an auditable checksum database which will be used by the go command to authenticate modules.","title":"What are Go Modules?"},{"location":"using-go-modules/#why-go-modules","text":"Go Modules bring package versioning to Go. Versioning enables reproducible builds by ensuring that a program builds exactly the same way tomorrow as it does today. Modules also adopt semantic versioning, eliminates vendoring, and deprecates $GOPATH in favor of a project-based workflow. Previously, when go get needed a package, it always fetched the latest copy, delegating the download and update operations to version control systems like Git. This meant that without a concept of versioning, go get could not convey to users any expectations about what kind of changes to expect in a given update. With versioning, library writers and users can frame the expectation around updates. Consequently there is a clear expectation on breaking updates versus non-breaking updates.","title":"Why Go Modules?"},{"location":"using-go-modules/#why-semantic-import-versioning","text":"Go Modules champion the Import Compatibility Rule , where the expectation is that newer versions of a package, with a given import path, should be backwards-compatible with older versions. If an old package and a new package have the same import path, then the new package must be backwards compatible with the old package. ex: \"github.com/foo/foo_package@v1.2.3\" => \"github.com/foo/foo_package@1.2.5\" If a breaking change is required, then a new package should be created with a new import path. ex: \"github.com/foo/foo_package@v1.2.3\" => \"github.com/foo/foo_package/v2@v2.0.0\" Because each major version has a different import path, a module can contain one of each major version of a package. This means modules need to specify only the minimum requirements for their dependencies. Further, since both v1 and v2 of a package can coexist, this makes it easy to upgrade the clients of a package one at a time while guaranteeing successful builds. v0 is special because of its special role in Semantic Versioning Precedence Rules where major versions >=1 have different import paths.","title":"Why Semantic Import Versioning?"},{"location":"using-go-modules/#how-are-the-module-versions-to-use-in-each-build-determined","text":"Go Modules use the Minimal Version Selection (MVS) algorithm to determine which module versions to use in each build. A build of a module by itself uses the specific versions of required dependencies listed in the go.mod file. However, in larger builds, it only uses a newer version if a dependency in the build requires it. Minimal Version Selection always selects the minimal (oldest) module version that satisfies the overall requirements of a build. Thus, the release of a new version has no effect on the build. It assumes that each module declares its own dependency requirements (as a list of the minimum versions of other modules) It assumes that modules follow the Import Compatibility Rule where packages in newer versions should work as well as the older versions. As such, the dependency requirement guarantees only a minimum version, never a maximum version or a list of incompatible later versions.","title":"How are the module versions to use in each build determined?"},{"location":"using-go-modules/#how-does-the-minimal-version-selection-mvs-algorithm-construct-the-build-list-for-a-given-module","text":"To construct the build list for a given module, the MVS algorithm starts the list with the module itself, and then appends each dependency's own build list. If a module appears in the list multiple times, it keeps only the newest module version. The build list is constructed using a graph reachability algorithm, with the given module as the starting node. The algorithm recursively traverses the graph, taking care not to visit a node that has already been visited. Once the traversal is complete (once it has a rough build list of nodes, including nodes with multiple versions), it computes the final build list by keeping only the newest version of any listed module. All other versions in the rough build list are discarded.","title":"How does the Minimal Version Selection (MVS) algorithm construct the build list for a given module?"},{"location":"using-go-modules/#how-does-the-mvs-algorithm-upgrade-all-modules-to-their-latest-version","text":"To upgrade all modules to their latest versions, the MVS algorithm computes an upgraded build list by updating the module requirement graph so that every outbound link to any version of a module is replaced with one that links to the latest version of that module. It then uses the previous algorithm to determine the final build list. Commands: go get -u (without any arguments) => upgrade direct and indirect dependencies of the current package go get -u ./... (from the module root) => upgrade all direct and indirect dependencies of your module, excluding test dependencies go get -u=patch ./... (from the module root) => same as above, using the latest patch release go get -u -t ./... (from the module root) => upgrade all direct and indirect dependencies of your module, including test dependencies go get -u=patch -t ./... (from the module root) => same as above, using the latest patch release go get -u all => update all of the packages transitively imported by the main module (including test dependencies) go get -u github.com/foo => get the latest version of foo as well as the latest versions for all of the direct and indirect dependencies of foo","title":"How does the MVS algorithm upgrade all modules to their latest version?"},{"location":"using-go-modules/#how-does-the-mvs-algorithm-upgrade-one-module-to-a-specific-newer-version","text":"To upgrade one module to a specific newer version, it first constructs the build list of the non-upgraded module as before. Then it appends the new module's build list. If a module appears in the list multiple times, it keeps only the newest module version. Commands: go get foo@v1.2.3 => get a specific version go get foo@latest => get the latest version go get foo@master => using a branch name such as @master obtains the latest commit regardless of whether or not it has a semantic versioning tag Common Workflow A common starting point when upgrading foo is: - `go get foo` or `go get foo@latest` and after things are working - `go get -u=patch foo` to upgrade all direct and indirect dependencies of `foo`","title":"How does the MVS algorithm upgrade one module to a specific newer version?"},{"location":"using-go-modules/#how-does-the-mvs-algorithm-downgrade-one-module-to-a-specific-older-version","text":"To downgrade one module to a specific older version, it rewinds the required version of each top-level dependency, until that dependency's build list no longer refers to newer versions of the downgraded module. Downgrading one module may require downgrading other modules, so the algorithm tries to downgrade as few other modules as possible. If a requirement is incompatible with the proposed downgrade (if the requirement's build list includes a now-disallowed module version), then it successively tries older versions until it finds one that is compatible with the downgrade. Commands: go get foo@v1.2.3 => downgrade the foo package to the specified version","title":"How does the MVS algorithm downgrade one module to a specific older version?"},{"location":"using-go-modules/#what-advantage-does-this-have-over-systems-that-use-lock-files","text":"Because a module's dependency requirements are included with the module's source code to uniquely determine how to build it, and because the Minimal Version Selection algorithm provides high-fidelity builds by using the oldest version available that meets the dependency requirements, mechanisms such as lock files are not needed. In other systems, the lock file lists the specific versions a build should use. It only guarantees reproducible builds for whole programs, not for library modules.","title":"What advantage does this have over systems that use lock files?"},{"location":"using-go-modules/#why-does-using-go-modules-make-vendoring-unnecessary","text":"Previously, if you were using an external package and worried that it might change in unexpected ways, you copied it to your local repository, and stored the copy under a new import path that identified it as a local copy, for example: github.com/pkg => my_local_path/external/github.com/pkg Over time, this was encapsulated in tools that copied a dependency into your repository and also updated all the import paths wihin it to reflect the new location. However, these modifications made it harder to compare against, and even incorporate newer copies, required updates, etc., to other code using that dependency. Package managers like go dep evolved to become a tool that could be used for freezing external package dependencies. This introduced the concept of vendoring , where you could copy dependencies into the project without modifying the source files as long as the GOPATH was correctly set up. But these vendoring tools relied on metadata files such as glide.yml or dep.yml to provide reproducible builds, and could not help with deciding which versions of a package to use. Vendor directories: specify by their contents, the exact version of the dependencies to use when running go build . ensure the availability of those dependencies, even if the original copies disappear. are unwieldly and bloat the repositories in which they are used. GOPATH was used to: define the versions of dependencies. hold the source code for those dependencies. provide a way to infer the import path for code in a specific directory. With Go Modules, GOPATH s are no longer required because the go.mod file includes the full module path and defines the version of each dependency in use. Further, since a directory with a go.mod file marks the root of the directory tree, this directory serves as a self-contained workspace, and is separate from all other directories.","title":"Why does using Go Modules make vendoring unnecessary?"},{"location":"using-go-modules/#why-are-pseudo-versions-used-to-identify-untagged-commits","text":"To name untagged commits, Go Modules use the pseudo-version format v0.0.0-yyyymmddhhmmss-{commit identifier} to identify a specific commit made on a given date. The commit identifier is typically a shorted Git hash, and must have a commit time matching the UTC timestamp. The pseudo-version form is used so that Semantic Versioning Precedence Rules can be used to compare two pseudo-versions by commit time, since the timestamp encoding makes string comparison match time comparison. This also ensures that go mod always prefers a tagged semantic version(even if it's very old, such as v0.0.1 ), over an untagged pseudo-version, since it has a greater semantic version precedence than any v0.0.0 pre-release","title":"Why are pseudo-versions used to identify untagged commits?"},{"location":"using-go-modules/#can-i-use-go-modules-along-with-the-traditional-gopath-based-mechanisms","text":"Currently, the Go command defaults to module mode when run in directories outside GOPATH/src that are marked by go.mod files in their roots. This can be overriden by setting the transitional environment variable $GO111MODULE to on or off . The default mode is auto .","title":"Can I use Go Modules along with the traditional GOPATH-based mechanisms?"},{"location":"using-go-modules/#how-can-i-make-the-current-directory-the-root-of-a-module","text":"go mod init : when working in a directory outside $GOPATH . This command writes a go.mod file at the root of your directory","title":"How can I make the current directory, the root of a module?"},{"location":"using-go-modules/#can-packages-also-have-gomod-files","text":"The go.mod file only appears in the root of the module. Packages in subdirectories have import paths consisting of the module path and the path to that subdirectory.","title":"Can packages also have go.mod files?"},{"location":"using-go-modules/#what-happens-if-a-package-is-not-provided-by-any-module-in-gomod","text":"The Go command automatically looks up the module containing that package and adds it to go.mod using the latest version. Latest is defined as the latest tagged stable version, or else, the latest tagged pre-release version, or else, the latest untagged version. Only direct dependencies are recorded in the go.mod file.","title":"What happens if a package is not provided by any module in go.mod?"},{"location":"using-go-modules/#can-modules-be-cached-locally","text":"Yes, they are cached locally in $GOPATH/pkg/mod","title":"Can modules be cached locally?"},{"location":"using-go-modules/#how-are-different-major-versions-of-package-distinguished","text":"Every major version of a Go Module {v1, v2, v3, ...} uses a different module path. Starting at v2 , the module path must end in the major version. Semantic Import Versioning is used to give incompatible packages(packages with different major versions) different names.","title":"How are different major versions of package distinguished?"},{"location":"using-go-modules/#can-i-build-a-program-using-different-minor-versions","text":"No, since these are considered duplicates of a single module path. Go Modules allow a build to include at most one version of any particular module path, which means, at most, one of each major version. Allowing different major versions of a module (since they have different paths) enables incremental upgrades to a new major version.","title":"Can I build a program using different minor versions?"},{"location":"using-go-modules/#how-do-i-clean-up-unused-dependencies","text":"go mod tidy","title":"How do I clean up unused dependencies?"},{"location":"using-go-modules/#what-does-go-mod-tidy-do","text":"go mod tidy finds all the packages transitively imported by packages in your module and adds new module requirements for packages not provided by any known modules, while removing requirements on modules that do not provide any imported packages. If a module provides packages that are only imported by projects that have not yet migrated to modules, it marks that module requirement with an // indirect comment.","title":"What does go mod tidy do?"},{"location":"using-go-modules/#i-get-errors-when-i-run-go-mod-tidy-what-could-it-be","text":"When go mod tidy adds a requirement, it adds the latest version of the module. However, if your $GOPATH included an older version of a dependency that subsequently published a breaking change, go mod tidy will be unable to resolve the right package. Run go mod graph to get a graph of the module dependency requirements and then try downgrading to an older version with the go get command.","title":"I get errors when I run go mod tidy, what could it be?"},{"location":"using-go-modules/#how-can-i-fix-tests-that-are-failing-because-they-cannot-write-files-in-the-package-directory","text":"Tests may fail when the package directory is in the module cache, which has read-only access. Configure your tests to copy the files they need to write to a temporary directory instead.","title":"How can I fix tests that are failing because they cannot write files in the package directory?"},{"location":"using-go-modules/#how-can-i-fix-tests-that-are-failing-because-they-rely-on-relative-paths-to-packages-in-another-module","text":"Tests that use relative paths to locate and read files in another package can fail if the package resides in a different module. You can either: copy the test inputs into your module convert the test inputs from raw files to data embedded in .go source files","title":"How can I fix tests that are failing because they rely on relative paths to packages in another module"},{"location":"using-go-modules/#how-can-i-fix-tests-that-are-failing-becase-they-expect-go-commands-within-the-test-to-run-in-gopath-mode","text":"Add a go.mod file to the source tree to be tested Set GO111MODULE=off","title":"How can I fix tests that are failing becase they expect go commands within the test to run in $GOPATH mode?"},{"location":"using-go-modules/#how-do-i-release-a-new-module-after-committing-my-changes","text":"tag your release git tag v1.2.3 publish your release with the same tag git push origin v1.2.3 The new go.mod file at the root of your module defines the canonical import path for the module and adds the new minimum version requirements","title":"How do I release a new module after committing my changes?"},{"location":"using-go-modules/#how-do-i-know-which-environment-variable-to-use-when-working-with-go-modules","text":"GO111MODULE={on|off} => transitional variable to toggle the use of Go Modules GOPRIVATE => controls which modules the Go command considers to be private (not available publicly) and should therefore not use the proxy or checksum database. ex: export GOPRIVATE=github.com/foo GONOPROXY => overrides GOPRIVATE for the specific decision of whether to use a proxy. ex: export GONOPROXY=github.com/foo GONOSUMDB => overrides GOPRIVATE for the specific decision of whether to use the checksum database. ex: export GONOSUMDB=github.com/foo","title":"How do I know which environment variable to use when working with Go Modules?"},{"location":"using-go-modules/#help-my-build-is-failing-and-i-dont-know-where-to-start","text":"Ensure that the following environment variables are appropriately set: GOPRIVATE , GONOPROXY , GONOSUMDB Run go mod tidy Run go mod graph to make sense of the module dependency requirements Apply your understanding of how dependency requirements are resolved based on the following scenarios: upgrading all modules upgrading one module downgrading one module If all else fails, someone in the #dev-go-talk may be able to help","title":"Help! My build is failing and I don't know where to start!"},{"location":"using-go-modules/#reference","text":"original series of articles on Go Modules, written by Russ Cox , the creator of Go Modules Go blog articles on Go Modules Need help? Post your question on these Slack channels: Dev Go Talk @ #dev-go-talk Did you find this page useful? Yes! No Maintained by: Kelvin Spencer","title":"Reference"}]}